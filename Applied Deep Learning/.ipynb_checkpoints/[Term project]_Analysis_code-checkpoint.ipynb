{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Deep Learning Term Project\n",
    "\n",
    "### Title & Objective: ㅁㅁㅁㅁㅁㅁㅁㅁㅁㅁ \n",
    "\n",
    "- Python code result\n",
    "\n",
    "- Hae-yong Joung (2019311266)\n",
    "\n",
    "- - - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from html_table_parser import parser_functions as parser\n",
    "import numpy as np\n",
    "import csv\n",
    "from konlpy.tag import Twitter \n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "import theano\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import sys, re\n",
    "import pandas as pd\n",
    "import csv\n",
    "import getpass\n",
    "import gensim\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers import Input, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D, Conv2D,Convolution2D, MaxPooling2D, MaxPooling1D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape, merge\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "okt = Okt()\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir('C:\\\\Users\\\\JYW\\\\OneDrive - 연세대학교 (Yonsei University)\\\\정해용\\\\2020 1학기\\\\딥러닝응용\\\\project')\n",
    "# should change the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - \n",
    "\n",
    "### 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>cPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019.04.01</td>\n",
       "      <td>{'깜짝 7K KIA 황인준에게 두번째 선발 기회가 올까', 'KIA 불펜 희망으로...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019.04.02</td>\n",
       "      <td>{'KIA 주간전망-호랑이 군단, 라이벌 삼성·키움과 정면대결', 'KIA 이명기의...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019.04.03</td>\n",
       "      <td>{\"또 다시 미뤄진 KIA '괴물루키' 김기훈 프로 데뷔 선발승, 삼성전 6이닝 4...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019.04.04</td>\n",
       "      <td>{\"KIA 선발 양현종 '역투'\", \"KIA 최형우, 적시타 '달려'\", 'KIA ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019.04.05</td>\n",
       "      <td>{\"KIA 이명기 '이게 홈런의 맛'\", \"'이명기 결승 홈런' KIA, 키움 꺾고...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title  cPOS\n",
       "0  2019.04.01  {'깜짝 7K KIA 황인준에게 두번째 선발 기회가 올까', 'KIA 불펜 희망으로...     1\n",
       "1  2019.04.02  {'KIA 주간전망-호랑이 군단, 라이벌 삼성·키움과 정면대결', 'KIA 이명기의...     0\n",
       "2  2019.04.03  {\"또 다시 미뤄진 KIA '괴물루키' 김기훈 프로 데뷔 선발승, 삼성전 6이닝 4...     0\n",
       "3  2019.04.04  {\"KIA 선발 양현종 '역투'\", \"KIA 최형우, 적시타 '달려'\", 'KIA ...     1\n",
       "4  2019.04.05  {\"KIA 이명기 '이게 홈런의 맛'\", \"'이명기 결승 홈런' KIA, 키움 꺾고...     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('C:\\\\Users\\\\JYW\\\\OneDrive - 연세대학교 (Yonsei University)\\\\정해용\\\\2020 1학기\\\\딥러닝응용\\\\project')\n",
    "\n",
    "dataset = pd.read_csv('newstitle_타이거즈.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>cPOS</th>\n",
       "      <th>title_tokken</th>\n",
       "      <th>title_imp_tokken</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019.04.01</td>\n",
       "      <td>{'깜짝 7K KIA 황인준에게 두번째 선발 기회가 올까', 'KIA 불펜 희망으로...</td>\n",
       "      <td>1</td>\n",
       "      <td>[({', Punctuation), (깜짝, Noun), (7, Number), (...</td>\n",
       "      <td>[깜짝, 황인준, 두번째, 선발, 기회, 불펜, 희망, 핫, 준영, 이번, 주, 승...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019.04.02</td>\n",
       "      <td>{'KIA 주간전망-호랑이 군단, 라이벌 삼성·키움과 정면대결', 'KIA 이명기의...</td>\n",
       "      <td>0</td>\n",
       "      <td>[({', Punctuation), (KIA, Alpha), (주간, Noun), ...</td>\n",
       "      <td>[주간, 전망, 호랑이, 군단, 라이벌, 삼성, 정면, 대결, 이명기, 역, 발상,...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019.04.03</td>\n",
       "      <td>{\"또 다시 미뤄진 KIA '괴물루키' 김기훈 프로 데뷔 선발승, 삼성전 6이닝 4...</td>\n",
       "      <td>0</td>\n",
       "      <td>[({\", Punctuation), (또, Noun), (다시, Noun), (미뤄...</td>\n",
       "      <td>[또, 다시, 괴물, 루키, 김기훈, 프로, 데뷔, 선발, 승, 성전, 이닝, 실점...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019.04.04</td>\n",
       "      <td>{\"KIA 선발 양현종 '역투'\", \"KIA 최형우, 적시타 '달려'\", 'KIA ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[({\", Punctuation), (KIA, Alpha), (선발, Noun), ...</td>\n",
       "      <td>[선발, 양현종, 역투, 최형우, 적시타, 선, 불, 리드, 오프, 적임, 누구, ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019.04.05</td>\n",
       "      <td>{\"KIA 이명기 '이게 홈런의 맛'\", \"'이명기 결승 홈런' KIA, 키움 꺾고...</td>\n",
       "      <td>1</td>\n",
       "      <td>[({\", Punctuation), (KIA, Alpha), (이명기, Noun),...</td>\n",
       "      <td>[이명기, 이, 홈런, 맛, 이명기, 결승, 홈런, 연패, 탈출, 안치홍, 안타, ...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title  cPOS  \\\n",
       "0  2019.04.01  {'깜짝 7K KIA 황인준에게 두번째 선발 기회가 올까', 'KIA 불펜 희망으로...     1   \n",
       "1  2019.04.02  {'KIA 주간전망-호랑이 군단, 라이벌 삼성·키움과 정면대결', 'KIA 이명기의...     0   \n",
       "2  2019.04.03  {\"또 다시 미뤄진 KIA '괴물루키' 김기훈 프로 데뷔 선발승, 삼성전 6이닝 4...     0   \n",
       "3  2019.04.04  {\"KIA 선발 양현종 '역투'\", \"KIA 최형우, 적시타 '달려'\", 'KIA ...     1   \n",
       "4  2019.04.05  {\"KIA 이명기 '이게 홈런의 맛'\", \"'이명기 결승 홈런' KIA, 키움 꺾고...     1   \n",
       "\n",
       "                                        title_tokken  \\\n",
       "0  [({', Punctuation), (깜짝, Noun), (7, Number), (...   \n",
       "1  [({', Punctuation), (KIA, Alpha), (주간, Noun), ...   \n",
       "2  [({\", Punctuation), (또, Noun), (다시, Noun), (미뤄...   \n",
       "3  [({\", Punctuation), (KIA, Alpha), (선발, Noun), ...   \n",
       "4  [({\", Punctuation), (KIA, Alpha), (이명기, Noun),...   \n",
       "\n",
       "                                    title_imp_tokken  length  \n",
       "0  [깜짝, 황인준, 두번째, 선발, 기회, 불펜, 희망, 핫, 준영, 이번, 주, 승...      18  \n",
       "1  [주간, 전망, 호랑이, 군단, 라이벌, 삼성, 정면, 대결, 이명기, 역, 발상,...      39  \n",
       "2  [또, 다시, 괴물, 루키, 김기훈, 프로, 데뷔, 선발, 승, 성전, 이닝, 실점...      30  \n",
       "3  [선발, 양현종, 역투, 최형우, 적시타, 선, 불, 리드, 오프, 적임, 누구, ...      32  \n",
       "4  [이명기, 이, 홈런, 맛, 이명기, 결승, 홈런, 연패, 탈출, 안치홍, 안타, ...      39  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec(open('DL_project_source.py').read()) # to use predetermined hard-coding functions \n",
    "\n",
    "dataset['title_tokken'] = dataset['title'].apply(tokenizer_okt_pos)\n",
    "dataset['title_imp_tokken'] = dataset['title_tokken'].apply(imp_tokken)\n",
    "dataset[\"length\"] = [len(str(dataset.title[i]).split(\",\")) for i in range(0,len(dataset))]\n",
    "\n",
    "count = Counter(list_appending(dataset['title_imp_tokken']))\n",
    "vocab = dict(count.most_common())\n",
    "num_words =  [len(word) for word in dataset.title_imp_tokken]\n",
    "\n",
    "max_l = np.max(num_words)\n",
    "max_s = np.max(dataset[\"length\"])\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded!\n",
      "number of documents: 183\n",
      "vocab size: 3495\n",
      "max article length: 296\n",
      "max article number: 76\n"
     ]
    }
   ],
   "source": [
    "print(\"data loaded!\")\n",
    "print(\"number of documents:\",str(len(dataset)))\n",
    "print(\"vocab size:\",str(len(vocab)))\n",
    "print(\"max article length:\",str(max_l))\n",
    "print(\"max article number:\",str(max_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Making embedding matrix and index with pretrained korean word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30185 words vectors in Korean word2vec bin file\n"
     ]
    }
   ],
   "source": [
    "w2v_file = \".//ko.bin\"\n",
    "\n",
    "ko_model = gensim.models.Word2Vec.load(w2v_file)\n",
    "print('%s words vectors in Korean word2vec bin file' % len(ko_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedding matrix shape:  (3496, 200)\n"
     ]
    }
   ],
   "source": [
    "w2v_original = load_bin_vec_kor(w2v_file, vocab)\n",
    "\n",
    "add_unknown_words(w2v_original, vocab) # add undedifined word in w2v model\n",
    "W, word_idx_map = get_W(w2v_original)\n",
    "\n",
    "print('Word embedding matrix shape: ',W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split train and valid dataset\n",
    "- Train : 150 / Valid : 33 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making word index dataset and labels\n",
    "\n",
    "X, Y = [], []\n",
    "filter_h = 3\n",
    "pad = filter_h - 1\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    status = dataset[\"title_imp_tokken\"][idx]\n",
    "\n",
    "    words = status\n",
    "    words_set = set(words)\n",
    "    y=[]\n",
    "    for i in range(pad):\n",
    "        y.append(0)\n",
    "    for word in words_set:\n",
    "        if word in word_idx_map:\n",
    "            y.append(word_idx_map[word])\n",
    "\n",
    "    while len(y) < max_l+pad:\n",
    "        y.append(0)\n",
    "\n",
    "    X.append(y)\n",
    "    Y.append(dataset[\"cPOS\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(Y)\n",
    "\n",
    "indices = np.arange(dataset.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X = X.iloc[indices]\n",
    "Y = Y.iloc[indices]\n",
    "\n",
    "x_train = X[:150]\n",
    "y_train = Y[:150]\n",
    "x_val = X[150:183]\n",
    "y_val = Y[150:183]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 150\n",
      "Valid set: 33\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set:\",str(len(x_train)))\n",
    "print(\"Valid set:\",str(len(x_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - \n",
    "### 2. Original Model Architecture\n",
    "\n",
    "- Using 3 n-gram filters (n=1,2,3)\n",
    "- Loss function : Negative log likelihood (binary cross-entropy)\n",
    "- Optimizer : Adadelta\n",
    "- Metrics : Accuracy\n",
    "- Epochs : 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples, validate on 33 samples\n",
      "Epoch 1/50\n",
      "150/150 [==============================] - 5s 31ms/step - loss: 0.6986 - accuracy: 0.5667 - val_loss: 0.7097 - val_accuracy: 0.5152\n",
      "Epoch 2/50\n",
      "150/150 [==============================] - 4s 29ms/step - loss: 0.6479 - accuracy: 0.6467 - val_loss: 0.6898 - val_accuracy: 0.5152\n",
      "Epoch 3/50\n",
      "150/150 [==============================] - 4s 28ms/step - loss: 0.5815 - accuracy: 0.7267 - val_loss: 0.7741 - val_accuracy: 0.4848\n",
      "Epoch 4/50\n",
      "150/150 [==============================] - 4s 28ms/step - loss: 0.4936 - accuracy: 0.8267 - val_loss: 0.7917 - val_accuracy: 0.4848\n",
      "Epoch 5/50\n",
      "150/150 [==============================] - 4s 28ms/step - loss: 0.4280 - accuracy: 0.8800 - val_loss: 1.0651 - val_accuracy: 0.5152\n",
      "Epoch 6/50\n",
      "150/150 [==============================] - 5s 30ms/step - loss: 0.3273 - accuracy: 0.9533 - val_loss: 0.7226 - val_accuracy: 0.5455\n",
      "Epoch 7/50\n",
      "150/150 [==============================] - 4s 28ms/step - loss: 0.1797 - accuracy: 1.0000 - val_loss: 0.8403 - val_accuracy: 0.5152\n",
      "Epoch 8/50\n",
      "150/150 [==============================] - 5s 30ms/step - loss: 0.1287 - accuracy: 1.0000 - val_loss: 0.8441 - val_accuracy: 0.6061\n",
      "Epoch 9/50\n",
      "150/150 [==============================] - 5s 30ms/step - loss: 0.0643 - accuracy: 1.0000 - val_loss: 0.7699 - val_accuracy: 0.5152\n",
      "Epoch 10/50\n",
      "150/150 [==============================] - 4s 29ms/step - loss: 0.0405 - accuracy: 1.0000 - val_loss: 0.8393 - val_accuracy: 0.6364\n",
      "Epoch 11/50\n",
      "150/150 [==============================] - 4s 30ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 0.8042 - val_accuracy: 0.5455\n",
      "Epoch 12/50\n",
      "150/150 [==============================] - 4s 29ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.8318 - val_accuracy: 0.6061\n",
      "Epoch 13/50\n",
      "150/150 [==============================] - 4s 29ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.8348 - val_accuracy: 0.6061\n",
      "Epoch 14/50\n",
      "150/150 [==============================] - 4s 30ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.8378 - val_accuracy: 0.5758\n",
      "Epoch 15/50\n",
      "150/150 [==============================] - 4s 30ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.8400 - val_accuracy: 0.5455\n",
      "Epoch 16/50\n",
      "150/150 [==============================] - 5s 32ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.8662 - val_accuracy: 0.6061\n",
      "Epoch 17/50\n",
      "150/150 [==============================] - 5s 33ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.8557 - val_accuracy: 0.5455\n",
      "Epoch 18/50\n",
      "150/150 [==============================] - 5s 32ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.8675 - val_accuracy: 0.5455\n",
      "Epoch 19/50\n",
      "150/150 [==============================] - 5s 31ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.8930 - val_accuracy: 0.6061\n",
      "Epoch 20/50\n",
      "150/150 [==============================] - 5s 32ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.8816 - val_accuracy: 0.5455\n",
      "Epoch 21/50\n",
      "150/150 [==============================] - 5s 31ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.8940 - val_accuracy: 0.5455\n",
      "Epoch 22/50\n",
      "150/150 [==============================] - 4s 30ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.9075 - val_accuracy: 0.5758\n",
      "Epoch 23/50\n",
      "150/150 [==============================] - 5s 30ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.9198 - val_accuracy: 0.5758\n",
      "Epoch 24/50\n",
      "150/150 [==============================] - 5s 31ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.9157 - val_accuracy: 0.5455\n",
      "Epoch 25/50\n",
      "150/150 [==============================] - 5s 31ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.9267 - val_accuracy: 0.4848\n",
      "Epoch 26/50\n",
      "150/150 [==============================] - 5s 31ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.9380 - val_accuracy: 0.5455\n",
      "Epoch 27/50\n",
      " 32/150 [=====>........................] - ETA: 3s - loss: 0.0020 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "input_size = max_l+pad # for Input layers\n",
    "max_vocab_size = len(vocab) + 1 # for Embedding layers\n",
    "\n",
    "main_input = Input(shape=(input_size,), dtype='int32')\n",
    "sequential_1 = Embedding(max_vocab_size, 200, input_length = max_l+2)(main_input)\n",
    "sequential_1 = Conv1D(200, 1, activation='relu')(sequential_1)\n",
    "sequential_1 = MaxPooling1D(1)(sequential_1)\n",
    "sequential_1 = Flatten()(sequential_1)\n",
    "\n",
    "sequential_2 = Embedding(max_vocab_size, 200, input_length = max_l+2)(main_input)\n",
    "sequential_2 = Conv1D(200, 2, activation='relu')(sequential_2)\n",
    "sequential_2 = MaxPooling1D(1)(sequential_2)\n",
    "sequential_2 = Flatten()(sequential_2)\n",
    "\n",
    "sequential_3 = Embedding(max_vocab_size, 200, input_length = max_l+2)(main_input)\n",
    "sequential_3 = Conv1D(200, 3, activation='relu')(sequential_3)\n",
    "sequential_3 = MaxPooling1D(1)(sequential_3)\n",
    "sequential_3 = Flatten()(sequential_3)\n",
    "\n",
    "merged = layers.concatenate([sequential_1, sequential_2, sequential_3])\n",
    "dense = Dense(100, activation='relu')(merged)\n",
    "outputs = Dense(1, activation='sigmoid')(dense)\n",
    "model = Model(inputs=main_input, outputs=outputs)    \n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualize loss function & accuracy by epoch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "np.mean(val_acc)\n",
    "\n",
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation Accuracy : \",np.max(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Update Model\n",
    "\n",
    "- Model looks like over-fitting. Let's try drop-out technique and use small epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = max_l+pad # for Input layers\n",
    "max_vocab_size = len(vocab) + 1 # for Embedding layers\n",
    "\n",
    "main_input = Input(shape=(input_size,), dtype='int32')\n",
    "sequential_1 = Embedding(max_vocab_size, 200, input_length = max_l+2)(main_input)\n",
    "sequential_1 = Conv1D(200, 1, activation='relu')(sequential_1)\n",
    "sequential_1 = MaxPooling1D(1)(sequential_1)\n",
    "sequential_1 = Flatten()(sequential_1)\n",
    "\n",
    "sequential_2 = Embedding(max_vocab_size, 200, input_length = max_l+2)(main_input)\n",
    "sequential_2 = Conv1D(200, 2, activation='relu')(sequential_2)\n",
    "sequential_2 = MaxPooling1D(1)(sequential_2)\n",
    "sequential_2 = Flatten()(sequential_2)\n",
    "\n",
    "sequential_3 = Embedding(max_vocab_size, 200, input_length = max_l+2)(main_input)\n",
    "sequential_3 = Conv1D(200, 3, activation='relu')(sequential_3)\n",
    "sequential_3 = MaxPooling1D(1)(sequential_3)\n",
    "sequential_3 = Flatten()(sequential_3)\n",
    "\n",
    "merged = layers.concatenate([sequential_1, sequential_2, sequential_3])\n",
    "merged = Dropout(0.5)(merged)\n",
    "dense = Dense(100, activation='relu')(merged)\n",
    "outputs = Dense(1, activation='sigmoid')(dense)\n",
    "model = Model(inputs=main_input, outputs=outputs)    \n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "np.mean(val_acc)\n",
    "\n",
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation Accuracy : \",np.max(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
